{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l9Dm2I70FPwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4069e6-8c1d-48b2-edcc-b00d39295372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10ms/step - accuracy: 0.2384 - loss: 2.7328\n",
            "Epoch 2/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - accuracy: 0.3747 - loss: 2.1343\n",
            "Epoch 3/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 8ms/step - accuracy: 0.4142 - loss: 1.9721\n",
            "Epoch 4/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.4388 - loss: 1.8857\n",
            "Epoch 5/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 8ms/step - accuracy: 0.4548 - loss: 1.8126\n",
            "Epoch 6/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - accuracy: 0.4736 - loss: 1.7409\n",
            "Epoch 7/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.4810 - loss: 1.7097\n",
            "Epoch 8/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.4888 - loss: 1.6789\n",
            "Epoch 9/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - accuracy: 0.4935 - loss: 1.6532\n",
            "Epoch 10/10\n",
            "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.5012 - loss: 1.6209\n",
            "shall i compare thee to a summer's day?\n",
            "then sweets poaised and with to strie,\n",
            "ong week a live, thy have and that love stake.\n",
            "  for wration the says nothers unkined that live\n",
            "by boor that i me thou frower of me make\n",
            "stoe bettain, and rass \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import random\n",
        "\n",
        "# Load text dataset\n",
        "with open(\"/content/drive/MyDrive/sonnets.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(text)\n",
        "total_chars = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Generate character sequences\n",
        "sequences = []\n",
        "sequence_length = 40  # Reduced sequence length to speed up training\n",
        "for i in range(sequence_length, len(text)):\n",
        "    seq = text[i-sequence_length:i]\n",
        "    sequences.append([tokenizer.word_index[char] for char in seq if char in tokenizer.word_index])\n",
        "\n",
        "# Ensure sequences are not empty\n",
        "if len(sequences) == 0:\n",
        "    raise ValueError(\"No valid sequences generated. Check text preprocessing.\")\n",
        "\n",
        "# Convert to NumPy array and reshape\n",
        "data = np.array(sequences, dtype=object)\n",
        "data = np.stack(data)  # Ensures it's 2D\n",
        "\n",
        "# Prepare input and output\n",
        "x, y = data[:, :-1], data[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_chars)\n",
        "x = pad_sequences(x, maxlen=sequence_length - 1, padding='pre')\n",
        "\n",
        "# Define optimized LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_chars, output_dim=32, input_length=sequence_length - 1),  # Reduced output_dim\n",
        "    LSTM(64, return_sequences=True),  # Reduced LSTM units\n",
        "    LSTM(64),\n",
        "    Dense(total_chars, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model with fewer epochs and smaller batch size\n",
        "model.fit(x, y, epochs=10, batch_size=32, verbose=1)  # Reduced epochs & batch size\n",
        "\n",
        "# Text Generation function\n",
        "def generate_text(seed_text, next_chars=100, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(next_chars):\n",
        "        seq = [tokenizer.word_index[char] for char in generated_text[-(sequence_length - 1):] if char in tokenizer.word_index]\n",
        "        seq = pad_sequences([seq], maxlen=sequence_length - 1, padding='pre')\n",
        "        preds = model.predict(seq, verbose=0)[0]\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        next_index = np.random.choice(range(total_chars), p=preds)\n",
        "        next_char = tokenizer.index_word.get(next_index, '')\n",
        "        generated_text += next_char\n",
        "    return generated_text\n",
        "\n",
        "# Generate sample text\n",
        "seed = \"shall i compare thee to a summer's day?\"\n",
        "print(generate_text(seed, next_chars=200, temperature=0.8))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mshvv65WFijo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}